# core/analyses.py

import logging
import math

logger = logging.getLogger(__name__)

def analyze_rule_complexity(rule_func, conditions, sequence_constraints):
    """
    Analyzes the complexity of a generated rule function.

    :param rule_func: The function generated by `sequence_rule_generator`
    :param conditions: The logical conditions used to generate the rule
    :param sequence_constraints: The structural constraints on sequences
    :return: Dictionary of complexity metrics
    """
    logger.info("Starting complexity analysis")

    # Special case for empty conditions - all metrics should indicate minimum complexity
    if not conditions:
        return {
            "condition_count": 0,
            "sequence_length": len(sequence_constraints),
            "logical_depth": 0,
            "entropy": 0.0,
            "branching_factor": 0,
            "redundancy": 0.0,  # No redundancy when there are no conditions
            "execution_cost": 0,
            "contradictions": 0,
            "categorical_constraints": 0,
            "numerical_constraints": 0,
            "condition_impact": 0.0
        }

    complexity = {
        "condition_count": len(conditions),
        "sequence_length": len(sequence_constraints),
        "logical_depth": sum(1 for cond in conditions if isinstance(cond, tuple)),
        "entropy": compute_entropy(conditions),
        "branching_factor": compute_branching_factor(conditions),
        "redundancy": compute_redundancy(conditions, consider_properties=False),
        "execution_cost": estimate_execution_cost(conditions, sequence_constraints),
        "contradictions": detect_contradictions(conditions),
        "categorical_constraints": count_categorical_constraints(conditions),
        "numerical_constraints": count_numerical_constraints(conditions),
        "condition_impact": compute_condition_impact(conditions, sequence_constraints)
    }

    logger.info(f"Complexity analysis results: {complexity}")
    return complexity

def compute_entropy(conditions):
    """
    Computes a basic measure of rule expressiveness.

    :param conditions: The conditions used in the rule
    :return: A numerical measure of rule entropy
    """
    logger.debug(f"Computing entropy for conditions: {conditions}")

    if not conditions:
        return 0.0  # No entropy in empty conditions

    try:
        # First check for invalid conditions
        for _, _, value in conditions:
            if value is None or isinstance(value, dict):
                msg = "Invalid condition value found (None or dict), returning 0.0"
                logger.debug(msg)
                return 0.0

        # Convert conditions to hashable format
        hashable_conditions = []
        for prop, op, value in conditions:
            # Special handling for lists
            if isinstance(value, list):
                if not value:
                    # For empty lists, only consider operator
                    hashable_conditions.append((op, "EMPTY_LIST"))
                else:
                    # For non-empty lists, include property name
                    hashable_conditions.append((prop, op, tuple(sorted(value))))
            else:
                # For non-list values, include property name
                hashable_conditions.append((prop, op, value))

        # Count unique conditions
        unique_conditions = set(hashable_conditions)
        # Normalized complexity
        entropy = len(unique_conditions) / len(conditions)
        logger.debug(f"Calculated entropy: {entropy}")
        return entropy

    except (TypeError, ValueError):
        # If we can't compute entropy, return 0.0
        logger.debug("Failed to compute entropy, returning 0.0")
        return 0.0

def compute_branching_factor(conditions):
    """
    Computes the branching factor, representing the number of decision points.

    :param conditions: The conditions used in the rule
    :return: Numerical measure of rule branching complexity
    """
    logger.debug(f"Computing branching factor for conditions: {conditions}")

    # Check for invalid conditions first
    for _, _, value in conditions:
        if value is None or isinstance(value, dict):
            logger.debug("Invalid condition value found (None or dict), returning 0.0")
            return 0.0

    # Range-based conditions
    range_operators = {">", "<", ">=", "<="}
    branching = sum(1 for cond in conditions if cond[1] in range_operators)
    logger.debug(f"Calculated branching factor: {branching}")
    return branching

def compute_redundancy(conditions, consider_properties=True):
    """
    Measures redundancy by analyzing repeated structural patterns.

    :param conditions: The conditions used in the rule
    :param consider_properties: Whether to consider property names in redundancy calculation
    :return: Ratio of redundant conditions
    """
    logger.debug(f"Computing redundancy for conditions: {conditions}")

    if not conditions:
        return 1.0  # Maximum redundancy when no conditions exist

    # Count occurrences of patterns
    pattern_counts = {}
    for prop, op, _ in conditions:
        pattern = (prop, op) if consider_properties else op
        pattern_counts[pattern] = pattern_counts.get(pattern, 0) + 1

    # Calculate redundancy based on unique patterns
    total_patterns = len(conditions)
    unique_patterns = len(pattern_counts)
    redundancy = 1 - (unique_patterns / total_patterns)

    logger.debug(f"Calculated redundancy: {redundancy}")
    return redundancy

def estimate_execution_cost(conditions, sequence_constraints):
    """
    Estimates the execution cost of a rule based on its complexity.

    :param conditions: The conditions used in the rule
    :param sequence_constraints: The structural constraints on sequences
    :return: Execution cost score
    """
    logger.debug(
        f"Estimating execution cost for conditions: {conditions} "
        f"and sequence constraints: {sequence_constraints}"
    )
    cost = len(conditions) * len(sequence_constraints)  # Naive cost estimate
    logger.debug(f"Estimated execution cost: {cost}")
    return cost

def detect_contradictions(conditions):
    """
    Detects if there are conflicting conditions (e.g., rank > 7 and rank < 7).

    :param conditions: The conditions used in the rule
    :return: Count of contradictions
    """
    logger.debug(f"Checking for contradictions in conditions: {conditions}")
    seen = {}
    contradictions = 0

    for prop, op, value in conditions:
        if prop in seen:
            prev_op, prev_value = seen[prop]
            if (prev_op == ">" and op == "<" and prev_value >= value) or \
               (prev_op == "<" and op == ">" and prev_value <= value) or \
               (prev_op == "=" and op != "=") or \
               (op == "=" and prev_op != "="):
                contradictions += 1
                logger.warning(
                    f"Contradiction found: {prop} {prev_op} {prev_value} "
                    f"vs {prop} {op} {value}"
                )
        seen[prop] = (op, value)

    logger.debug(f"Detected {contradictions} contradictions")
    return contradictions

def count_categorical_constraints(conditions):
    """Counts conditions that apply to categorical (non-numeric) properties."""
    return sum(1 for _, _, value in conditions if isinstance(value, str))

def count_numerical_constraints(conditions):
    """Counts conditions that apply to numeric properties."""
    return sum(1 for _, _, value in conditions if isinstance(value, (int, float)))

def compute_condition_impact(conditions, sequence_constraints):
    """
    Estimates how impactful each condition is in determining complexity.

    :param conditions: List of conditions [(property, operator, value), ...]
    :param sequence_constraints: Ordered list of required objects
    :return: Impact score (higher = more complex)
    """
    logger.debug(f"Computing condition impact for {conditions}")

    impact = sum(
        (1.5 if op in {"<", ">"} else 1.2 if op in {"<=", ">="} else 1.0)
        for _, op, _ in conditions
    )

    impact /= max(1, len(sequence_constraints))  # Normalize by sequence length

    logger.debug(f"Condition impact score: {impact}")
    return impact

def compute_custom_complexity_score(complexity_metrics, model="weighted", weights=None):
    """
    Computes a complexity score using different models.

    :param complexity_metrics: Dictionary of complexity values
    :param model: The scoring model ('weighted', 'entropy_based', 'log_scaled', 'normalized')
    :param weights: Optional dictionary of weights for 'weighted' model
    :return: Computed complexity score
    """
    logger.debug(f"Computing complexity score using model: {model}")

    # Only return 0.0 if entropy is explicitly set to 0.0
    if "entropy" in complexity_metrics and complexity_metrics["entropy"] == 0.0:
        logger.debug("Entropy is explicitly set to 0.0, returning complexity score of 0.0")
        return 0.0

    if model == "weighted":
        if weights is None:
            weights = {key: 1.0 for key in complexity_metrics}
        score = sum(
            complexity_metrics[key] * weights[key]
            for key in complexity_metrics if key in weights
        )

    elif model == "entropy_based":
        # If entropy is not present, use a default value of 1.0
        score = complexity_metrics.get("entropy", 1.0) * 10  # Scale entropy to a 0-10 range

    elif model == "log_scaled":
        # Logarithmic scaling
        score = sum(
            math.log1p(complexity_metrics[key])
            for key in complexity_metrics
        )

    elif model == "normalized":
        max_values = {key: max(1, complexity_metrics[key]) for key in complexity_metrics}
        score = sum(complexity_metrics[key] / max_values[key] for key in complexity_metrics)

    else:
        raise ValueError(f"Invalid complexity model: {model}")

    logger.debug(f"Computed complexity score ({model}): {score}")
    return score
